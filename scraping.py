# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1h1fhyRMtQ-A_hO_76ORSC8LK39Zw7QbF
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time

def scrape_trt_news():
    # URL of the news website
    url = "https://www.trthaber.com"

    # Fetch the homepage
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')

    # Find the news cards
    news_cards = soup.find_all('div', class_='standard-card')

    # Create DataFrame
    df = pd.DataFrame(columns=['link', 'title', 'text'])

    for card in news_cards:
        # Extract the link and title
        link_tag = card.find('a', class_='site-url')
        if link_tag:
            link = link_tag['href']
            title = link_tag['title']

            # Fetch the news article page
            article_response = requests.get(link)
            article_soup = BeautifulSoup(article_response.content, 'html.parser')

            # Extract the news content
            news_content = article_soup.find('div', class_='news-content')
            if news_content:
                paragraphs = news_content.find_all('p')
                text = '\n'.join(paragraph.text for paragraph in paragraphs)
            else:
                text = 'Content not found'

            # Append data to DataFrame
            df.loc[len(df)] = [link, title, text]


    df = df[df['text'] != 'Content not found']

    return df


def get_article_info(article_card):
    headline = article_card.find('span', class_='container__headline-text').get_text()
    link = article_card.find('a', href=True)
    if link:
        url = 'https://edition.cnn.com' + link['href']
    else:
        url = None
    return headline, url

def get_article_content(article_url):
    try:
        response = requests.get(article_url)
        if response.status_code != 200:
            print(f"Failed to retrieve the article page: {article_url}")
            return ""

        soup = BeautifulSoup(response.content, 'html.parser')
        content_div = soup.find('div', class_='article__content')

        paragraphs = content_div.find_all('p') if content_div else []
        article_content = "\n".join(paragraph.get_text() for paragraph in paragraphs)

        return article_content
    except Exception as e:
        print(f"An error occurred: {e}")
        return ""

def scrape_cnn_articles(max_articles=20):
    main_url = "https://edition.cnn.com/"
    response = requests.get(main_url)
    if response.status_code != 200:
        print("Failed to retrieve the main page")
        return pd.DataFrame()

    soup = BeautifulSoup(response.content, 'html.parser')
    article_cards = soup.find_all('div', {'data-component-name': 'card'})

    articles = []
    num_articles = 0
    index = 0
    while num_articles < max_articles and index < len(article_cards):
        card = article_cards[index]
        headline, url = get_article_info(card)
        if url:
            article_content = get_article_content(url)
            articles.append({
                'headline': headline,
                'url': url,
                'content': article_content
            })
            num_articles += 1
            time.sleep(3)  # Add a delay of 3 seconds after scraping each article
        index += 1

    df = pd.DataFrame(articles)
    return df



    